# é«˜çº§åˆ†ææŒ‡å—

æœ¬æŒ‡å—ä»‹ç»lawkitçš„é«˜çº§åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤æ‚æ•°æ®åœºæ™¯ã€è‡ªå®šä¹‰åˆ†æå·¥ä½œæµå’Œä¸“ä¸šæŠ€æœ¯ã€‚

## é«˜çº§ç»Ÿè®¡åˆ†æ

### å¤šç»´åº¦åˆ†æ

```bash
# æ—¶é—´åºåˆ—åˆ†æ
lawkit normal daily_metrics.csv --enable-timeseries --timeseries-window 30

# å­£èŠ‚æ€§åˆ†è§£
lawkit normal monthly_sales.csv --enable-timeseries --seasonal-decomposition

# è¶‹åŠ¿æ£€æµ‹
lawkit normal stock_prices.csv --enable-timeseries --trend-detection --changepoint-analysis
```

### å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•

#### é›†æˆæ–¹æ³•ï¼ˆæ¨èï¼‰
```bash
# ä½¿ç”¨å¤šç§æ–¹æ³•çš„é›†æˆ
lawkit normal data.csv --outliers --outlier-method ensemble --verbose

# è¾“å‡ºè¯¦ç»†çš„å¼‚å¸¸å€¼ä¿¡æ¯
lawkit normal measurements.csv --outliers --outlier-method ensemble --format json | \
jq '.outliers[] | select(.confidence > 0.8)'
```

#### ç‰¹å®šæ–¹æ³•
```bash
# LOF (å±€éƒ¨å¼‚å¸¸å› å­) - é€‚ç”¨äºå¤æ‚æ¨¡å¼
lawkit normal complex_data.csv --outliers --outlier-method lof --neighbors 20

# éš”ç¦»æ£®æ— - é€‚ç”¨äºé«˜ç»´æ•°æ®
lawkit normal high_dim_data.csv --outliers --outlier-method isolation --contamination 0.1

# DBSCAN - é€‚ç”¨äºå¯†åº¦èšç±»
lawkit normal clustered_data.csv --outliers --outlier-method dbscan --eps 0.5 --min-samples 5
```

### å·¥è‰ºèƒ½åŠ›åˆ†æ

```bash
# å®Œæ•´çš„å·¥è‰ºèƒ½åŠ›åˆ†æ
lawkit normal production_data.csv --process-capability --control-limits --format json

# è‡ªå®šä¹‰è§„æ ¼é™åˆ¶
lawkit normal quality_data.csv --process-capability \
  --lower-spec-limit 95 --upper-spec-limit 105 --target 100

# Cpkå’ŒPpkè®¡ç®—
lawkit normal process_measurements.csv --process-capability --verbose | \
grep -E "Cpk|Ppk"
```

## å¤æ‚æ•°æ®åœºæ™¯

### æ··åˆåˆ†å¸ƒåˆ†æ

```bash
# æ£€æµ‹å¤šæ¨¡æ€åˆ†å¸ƒ
lawkit normal mixed_data.csv --multimodal-detection --verbose

# åˆ†ç¦»æ··åˆç»„ä»¶
lawkit normal bimodal_data.csv --mixture-analysis --components 2 --format json

# è‡ªåŠ¨ç¡®å®šç»„ä»¶æ•°
lawkit normal unknown_mixture.csv --mixture-analysis --auto-components
```

### ç¨€æœ‰äº‹ä»¶åˆ†æ

```bash
# æ³Šæ¾ç¨€æœ‰äº‹ä»¶æ£€æµ‹
lawkit poisson incident_data.csv --rare-events --confidence-level 0.99

# æå€¼ç†è®ºåˆ†æ
lawkit normal extreme_data.csv --extreme-value-analysis --threshold auto

# å°¾éƒ¨é£é™©è¯„ä¼°
lawkit normal financial_returns.csv --tail-risk --var-level 0.05 --cvar
```

### æ—¶é—´ç›¸å…³åˆ†æ

```bash
# å˜åŒ–ç‚¹æ£€æµ‹
lawkit normal time_series.csv --enable-timeseries --changepoint-detection \
  --method bayesian --sensitivity 0.8

# æ¼‚ç§»æ£€æµ‹
lawkit normal sensor_data.csv --enable-timeseries --drift-detection \
  --window-size 100 --threshold 0.05

# å‘¨æœŸæ€§åˆ†æ
lawkit normal seasonal_data.csv --enable-timeseries --periodicity-analysis \
  --max-period 365
```

## è‡ªå®šä¹‰åˆ†æå·¥ä½œæµ

### å¤šé˜¶æ®µåˆ†æç®¡é“

```bash
#!/bin/bash
# advanced_analysis_pipeline.sh

INPUT_FILE="$1"
OUTPUT_DIR="analysis_results"
mkdir -p "$OUTPUT_DIR"

echo "=== å¼€å§‹é«˜çº§åˆ†æç®¡é“ ==="
echo "è¾“å…¥æ–‡ä»¶: $INPUT_FILE"
echo "è¾“å‡ºç›®å½•: $OUTPUT_DIR"

# é˜¶æ®µ1: åˆæ­¥æ•°æ®è´¨é‡æ£€æŸ¥
echo "é˜¶æ®µ1: æ•°æ®è´¨é‡æ£€æŸ¥"
lawkit benf "$INPUT_FILE" --format json > "$OUTPUT_DIR/quality_check.json"

quality_risk=$(jq -r '.risk_level' "$OUTPUT_DIR/quality_check.json")
echo "æ•°æ®è´¨é‡é£é™©: $quality_risk"

if [ "$quality_risk" = "HIGH" ] || [ "$quality_risk" = "CRITICAL" ]; then
    echo "è­¦å‘Š: æ£€æµ‹åˆ°é«˜é£é™©æ•°æ®è´¨é‡é—®é¢˜"
    jq '.verdict' "$OUTPUT_DIR/quality_check.json"
fi

# é˜¶æ®µ2: åˆ†å¸ƒç±»å‹è¯†åˆ«
echo "é˜¶æ®µ2: åˆ†å¸ƒè¯†åˆ«"
lawkit analyze "$INPUT_FILE" --laws all --format json > "$OUTPUT_DIR/distribution_analysis.json"

best_fit=$(jq -r '.recommendations[0].law' "$OUTPUT_DIR/distribution_analysis.json")
echo "æœ€ä½³æ‹Ÿåˆåˆ†å¸ƒ: $best_fit"

# é˜¶æ®µ3: æ·±åº¦åˆ†æï¼ˆåŸºäºæœ€ä½³æ‹Ÿåˆï¼‰
echo "é˜¶æ®µ3: æ·±åº¦åˆ†æ"
case $best_fit in
    "normal")
        lawkit normal "$INPUT_FILE" --outliers --outlier-method ensemble \
          --quality-control --process-capability --enable-timeseries \
          --format json > "$OUTPUT_DIR/deep_analysis.json"
        ;;
    "pareto")
        lawkit pareto "$INPUT_FILE" --gini-coefficient --business-analysis \
          --percentiles "70,80,90,95,99" --format json > "$OUTPUT_DIR/deep_analysis.json"
        ;;
    "poisson")
        lawkit poisson "$INPUT_FILE" --predict --rare-events \
          --time-unit auto --format json > "$OUTPUT_DIR/deep_analysis.json"
        ;;
    *)
        lawkit "$best_fit" "$INPUT_FILE" --verbose --format json > "$OUTPUT_DIR/deep_analysis.json"
        ;;
esac

# é˜¶æ®µ4: å¼‚å¸¸æ£€æµ‹
echo "é˜¶æ®µ4: å¼‚å¸¸æ£€æµ‹"
lawkit normal "$INPUT_FILE" --outliers --outlier-method ensemble \
  --format json > "$OUTPUT_DIR/anomaly_detection.json"

anomaly_count=$(jq '.outliers | length' "$OUTPUT_DIR/anomaly_detection.json")
echo "æ£€æµ‹åˆ°å¼‚å¸¸å€¼æ•°é‡: $anomaly_count"

# é˜¶æ®µ5: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
echo "é˜¶æ®µ5: ç”ŸæˆæŠ¥å‘Š"
python3 <<EOF
import json
import datetime

# è¯»å–æ‰€æœ‰åˆ†æç»“æœ
with open('$OUTPUT_DIR/quality_check.json') as f:
    quality = json.load(f)
with open('$OUTPUT_DIR/distribution_analysis.json') as f:
    distribution = json.load(f)
with open('$OUTPUT_DIR/deep_analysis.json') as f:
    deep = json.load(f)
with open('$OUTPUT_DIR/anomaly_detection.json') as f:
    anomaly = json.load(f)

# ç”Ÿæˆç»¼åˆæŠ¥å‘Š
report = {
    "analysis_timestamp": datetime.datetime.now().isoformat(),
    "input_file": "$INPUT_FILE",
    "summary": {
        "data_quality_risk": quality.get('risk_level'),
        "best_fit_distribution": distribution.get('recommendations', [{}])[0].get('law'),
        "anomaly_count": len(anomaly.get('outliers', [])),
        "total_data_points": quality.get('numbers_analyzed')
    },
    "detailed_results": {
        "quality_assessment": quality,
        "distribution_comparison": distribution,
        "specialized_analysis": deep,
        "anomaly_detection": anomaly
    },
    "recommendations": []
}

# ç”Ÿæˆå»ºè®®
if quality.get('risk_level') in ['HIGH', 'CRITICAL']:
    report['recommendations'].append("æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æ”¶é›†è¿‡ç¨‹")

if len(anomaly.get('outliers', [])) > 0:
    report['recommendations'].append(f"æ£€æµ‹åˆ°{len(anomaly.get('outliers', []))}ä¸ªå¼‚å¸¸å€¼ï¼Œå»ºè®®è¿›ä¸€æ­¥è°ƒæŸ¥")

if distribution.get('conflicts'):
    report['recommendations'].append("æ£€æµ‹åˆ°åˆ†å¸ƒå†²çªï¼Œæ•°æ®å¯èƒ½æ¥è‡ªå¤šä¸ªæ¥æº")

# ä¿å­˜æŠ¥å‘Š
with open('$OUTPUT_DIR/comprehensive_report.json', 'w') as f:
    json.dump(report, f, indent=2, ensure_ascii=False)

print("ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ: $OUTPUT_DIR/comprehensive_report.json")
EOF

echo "=== åˆ†æç®¡é“å®Œæˆ ==="
echo "ç»“æœä¿å­˜åœ¨: $OUTPUT_DIR/"
```

### æ‰¹é‡åˆ†æè‡ªåŠ¨åŒ–

```bash
#!/bin/bash
# batch_advanced_analysis.sh

DATA_DIR="$1"
RESULTS_DIR="batch_analysis_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$RESULTS_DIR"

# æ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡ä»¶
find "$DATA_DIR" -name "*.csv" | xargs -P 8 -I {} bash -c "
    echo 'å¤„ç†æ–‡ä»¶: {}'
    
    # åˆ›å»ºæ–‡ä»¶ç‰¹å®šçš„ç»“æœç›®å½•
    file_base=\$(basename {} .csv)
    file_dir='$RESULTS_DIR/\$file_base'
    mkdir -p \"\$file_dir\"
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    ./advanced_analysis_pipeline.sh {} > \"\$file_dir/analysis.log\" 2>&1
    
    # ç§»åŠ¨ç»“æœåˆ°æ­£ç¡®ä½ç½®
    mv analysis_results/* \"\$file_dir/\" 2>/dev/null || true
    rmdir analysis_results 2>/dev/null || true
    
    echo 'å®Œæˆ: {}'
"

# ç”Ÿæˆæ‰¹é‡æ‘˜è¦æŠ¥å‘Š
python3 <<EOF
import json
import os
import glob

results_dir = "$RESULTS_DIR"
summary = {
    "batch_analysis_summary": {
        "total_files": 0,
        "high_risk_files": [],
        "anomaly_summary": {},
        "distribution_summary": {}
    }
}

for file_dir in glob.glob(f"{results_dir}/*/"):
    report_file = os.path.join(file_dir, "comprehensive_report.json")
    if os.path.exists(report_file):
        with open(report_file) as f:
            report = json.load(f)
        
        filename = os.path.basename(file_dir.rstrip('/'))
        summary["batch_analysis_summary"]["total_files"] += 1
        
        # æ”¶é›†é«˜é£é™©æ–‡ä»¶
        risk = report["summary"].get("data_quality_risk")
        if risk in ["HIGH", "CRITICAL"]:
            summary["batch_analysis_summary"]["high_risk_files"].append({
                "file": filename,
                "risk_level": risk
            })
        
        # ç»Ÿè®¡å¼‚å¸¸å€¼
        anomaly_count = report["summary"].get("anomaly_count", 0)
        if anomaly_count > 0:
            summary["batch_analysis_summary"]["anomaly_summary"][filename] = anomaly_count
        
        # ç»Ÿè®¡åˆ†å¸ƒç±»å‹
        dist = report["summary"].get("best_fit_distribution")
        if dist:
            if dist not in summary["batch_analysis_summary"]["distribution_summary"]:
                summary["batch_analysis_summary"]["distribution_summary"][dist] = 0
            summary["batch_analysis_summary"]["distribution_summary"][dist] += 1

# ä¿å­˜æ‰¹é‡æ‘˜è¦
with open(f"{results_dir}/batch_summary.json", "w") as f:
    json.dump(summary, f, indent=2, ensure_ascii=False)

print(f"æ‰¹é‡åˆ†æå®Œæˆï¼Œæ‘˜è¦ä¿å­˜åœ¨: {results_dir}/batch_summary.json")
print(f"å¤„ç†æ–‡ä»¶æ•°: {summary['batch_analysis_summary']['total_files']}")
print(f"é«˜é£é™©æ–‡ä»¶æ•°: {len(summary['batch_analysis_summary']['high_risk_files'])}")
EOF
```

## ä¸“ä¸šåˆ†ææŠ€æœ¯

### é‡‘èé£é™©åˆ†æ

```bash
# VaR (é£é™©ä»·å€¼) åˆ†æ
lawkit normal returns.csv --tail-risk --var-level 0.05 --format json | \
jq '.tail_risk.var_95'

# å‹åŠ›æµ‹è¯•
lawkit normal portfolio_values.csv --stress-test --scenarios extreme_market.json

# æµåŠ¨æ€§é£é™©åˆ†æ
lawkit poisson trading_volumes.csv --liquidity-analysis --time-unit minute
```

### è´¨é‡æ§åˆ¶é«˜çº§åŠŸèƒ½

```bash
# å¤šå˜é‡æ§åˆ¶å›¾
lawkit normal multivariate_quality.csv --multivariate-control-chart \
  --variables "temperature,pressure,flow_rate"

# å·¥è‰ºèƒ½åŠ›è¶‹åŠ¿åˆ†æ
lawkit normal daily_capability.csv --process-capability --trend-analysis \
  --time-column date --format json

# å…­è¥¿æ ¼ç›åˆ†æ
lawkit normal six_sigma_data.csv --six-sigma-analysis --defect-opportunity 1000000
```

### é¢„æµ‹åˆ†æ

```bash
# æ—¶é—´åºåˆ—é¢„æµ‹
lawkit normal time_series.csv --enable-timeseries --forecast \
  --forecast-horizon 30 --confidence-interval 0.95

# å¼‚å¸¸é¢„æµ‹
lawkit normal sensor_data.csv --anomaly-prediction --prediction-window 24 \
  --alert-threshold 0.8

# è¶‹åŠ¿å¤–æ¨
lawkit normal growth_data.csv --trend-extrapolation --extrapolation-periods 12
```

## å¯è§†åŒ–å’ŒæŠ¥å‘Š

### é«˜çº§å¯è§†åŒ–è„šæœ¬

```python
#!/usr/bin/env python3
# advanced_visualization.py

import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from scipy import stats
import argparse

def load_lawkit_results(filename):
    """åŠ è½½lawkitåˆ†æç»“æœ"""
    with open(filename, 'r') as f:
        return json.load(f)

def plot_benford_analysis(results, output_file):
    """ç»˜åˆ¶æœ¬ç¦å¾·åˆ†æå›¾"""
    digits = results['digits']
    
    digit_nums = list(range(1, 10))
    observed = [digits[str(d)]['observed'] for d in digit_nums]
    expected = [digits[str(d)]['expected'] for d in digit_nums]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # æŸ±çŠ¶å›¾æ¯”è¾ƒ
    x = np.arange(len(digit_nums))
    width = 0.35
    
    ax1.bar(x - width/2, observed, width, label='è§‚å¯Ÿå€¼', alpha=0.8)
    ax1.bar(x + width/2, expected, width, label='æœŸæœ›å€¼', alpha=0.8)
    ax1.set_xlabel('é¦–ä½æ•°å­—')
    ax1.set_ylabel('ç™¾åˆ†æ¯”')
    ax1.set_title('æœ¬ç¦å¾·åˆ†å¸ƒæ¯”è¾ƒ')
    ax1.set_xticks(x)
    ax1.set_xticklabels(digit_nums)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # åå·®å›¾
    deviations = [digits[str(d)]['deviation'] for d in digit_nums]
    colors = ['red' if abs(d) > 2 else 'blue' for d in deviations]
    
    ax2.bar(digit_nums, deviations, color=colors, alpha=0.7)
    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)
    ax2.axhline(y=2, color='red', linestyle='--', alpha=0.5, label='é˜ˆå€¼ Â±2%')
    ax2.axhline(y=-2, color='red', linestyle='--', alpha=0.5)
    ax2.set_xlabel('é¦–ä½æ•°å­—')
    ax2.set_ylabel('åå·® (%)')
    ax2.set_title('ä¸æœŸæœ›å€¼çš„åå·®')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"æœ¬ç¦å¾·åˆ†æå›¾ä¿å­˜ä¸º: {output_file}")

def plot_anomaly_detection(results, data_file, output_file):
    """ç»˜åˆ¶å¼‚å¸¸æ£€æµ‹å›¾"""
    # åŠ è½½åŸå§‹æ•°æ®
    data = pd.read_csv(data_file, header=None, names=['value'])
    
    # è·å–å¼‚å¸¸å€¼ç´¢å¼•
    outliers = results.get('outliers', [])
    outlier_indices = [o['index'] for o in outliers]
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    # æ—¶é—´åºåˆ—å›¾
    ax1.plot(data['value'], color='blue', alpha=0.7, label='æ­£å¸¸æ•°æ®')
    if outlier_indices:
        ax1.scatter(outlier_indices, data.iloc[outlier_indices]['value'], 
                   color='red', s=50, label='å¼‚å¸¸å€¼', zorder=5)
    ax1.set_title('å¼‚å¸¸å€¼æ£€æµ‹ç»“æœ')
    ax1.set_ylabel('æ•°å€¼')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ç®±çº¿å›¾
    box_data = [data[~data.index.isin(outlier_indices)]['value']]
    if outlier_indices:
        box_data.append(data.iloc[outlier_indices]['value'])
        labels = ['æ­£å¸¸æ•°æ®', 'å¼‚å¸¸å€¼']
    else:
        labels = ['æ•°æ®']
    
    ax2.boxplot(box_data, labels=labels)
    ax2.set_title('æ•°æ®åˆ†å¸ƒç®±çº¿å›¾')
    ax2.set_ylabel('æ•°å€¼')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"å¼‚å¸¸æ£€æµ‹å›¾ä¿å­˜ä¸º: {output_file}")

def plot_distribution_comparison(results, output_file):
    """ç»˜åˆ¶åˆ†å¸ƒæ¯”è¾ƒå›¾"""
    laws = results.get('analysis_results', {})
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    law_names = {
        'benford': 'æœ¬ç¦å¾·åˆ†å¸ƒ',
        'pareto': 'å¸•ç´¯æ‰˜åˆ†å¸ƒ', 
        'zipf': 'é½å¤«åˆ†å¸ƒ',
        'normal': 'æ­£æ€åˆ†å¸ƒ',
        'poisson': 'æ³Šæ¾åˆ†å¸ƒ'
    }
    
    for i, (law, result) in enumerate(laws.items()):
        if i >= len(axes):
            break
            
        ax = axes[i]
        
        # ç»˜åˆ¶é€‚åˆåº¦æŒ‡æ ‡
        if 'statistics' in result:
            stats_data = result['statistics']
            metrics = []
            values = []
            
            if 'chi_square' in stats_data:
                metrics.append('å¡æ–¹å€¼')
                values.append(stats_data['chi_square'])
            
            if 'p_value' in stats_data:
                metrics.append('på€¼')
                values.append(stats_data['p_value'])
            
            if 'mad' in stats_data:
                metrics.append('MAD')
                values.append(stats_data['mad'])
            
            if metrics:
                ax.bar(metrics, values, alpha=0.7)
                ax.set_title(f'{law_names.get(law, law)} - ç»Ÿè®¡æŒ‡æ ‡')
                ax.tick_params(axis='x', rotation=45)
                
        # é£é™©ç­‰çº§é¢œè‰²ç¼–ç 
        risk_level = result.get('risk_level', 'UNKNOWN')
        risk_colors = {'LOW': 'green', 'MEDIUM': 'yellow', 'HIGH': 'red', 'CRITICAL': 'darkred'}
        ax.set_facecolor(risk_colors.get(risk_level, 'white'))
        ax.set_alpha(0.1)
    
    # ç§»é™¤æœªä½¿ç”¨çš„å­å›¾
    for i in range(len(laws), len(axes)):
        fig.delaxes(axes[i])
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"åˆ†å¸ƒæ¯”è¾ƒå›¾ä¿å­˜ä¸º: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='é«˜çº§lawkitç»“æœå¯è§†åŒ–')
    parser.add_argument('result_file', help='lawkitåˆ†æç»“æœJSONæ–‡ä»¶')
    parser.add_argument('--data-file', help='åŸå§‹æ•°æ®æ–‡ä»¶ï¼ˆç”¨äºå¼‚å¸¸æ£€æµ‹å›¾ï¼‰')
    parser.add_argument('--output-dir', default='.', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åŠ è½½ç»“æœ
    results = load_lawkit_results(args.result_file)
    
    # æ ¹æ®åˆ†æç±»å‹ç”Ÿæˆç›¸åº”å›¾è¡¨
    base_name = args.result_file.replace('.json', '')
    
    if 'digits' in results:  # æœ¬ç¦å¾·åˆ†æ
        plot_benford_analysis(results, f"{args.output_dir}/{base_name}_benford.png")
    
    if 'outliers' in results and args.data_file:  # å¼‚å¸¸æ£€æµ‹
        plot_anomaly_detection(results, args.data_file, f"{args.output_dir}/{base_name}_anomalies.png")
    
    if 'analysis_results' in results:  # åˆ†å¸ƒæ¯”è¾ƒ
        plot_distribution_comparison(results, f"{args.output_dir}/{base_name}_comparison.png")

if __name__ == "__main__":
    main()
```

### HTMLæŠ¥å‘Šç”Ÿæˆ

```python
#!/usr/bin/env python3
# generate_html_report.py

import json
import jinja2
from datetime import datetime
import argparse

# HTMLæ¨¡æ¿
HTML_TEMPLATE = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>lawkit é«˜çº§åˆ†ææŠ¥å‘Š</title>
    <style>
        body { font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; }
        .header { background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }
        .summary { background: #ecf0f1; padding: 15px; margin: 20px 0; border-radius: 5px; }
        .section { margin: 20px 0; }
        .risk-low { color: green; font-weight: bold; }
        .risk-medium { color: orange; font-weight: bold; }
        .risk-high { color: red; font-weight: bold; }
        .risk-critical { color: darkred; font-weight: bold; }
        table { border-collapse: collapse; width: 100%; margin: 10px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .metric { display: inline-block; margin: 10px; padding: 10px; background: #f8f9fa; border-radius: 5px; }
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ” lawkit é«˜çº§æ•°æ®åˆ†ææŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {{ timestamp }}</p>
        <p>åˆ†ææ–‡ä»¶: {{ input_file }}</p>
    </div>

    <div class="summary">
        <h2>ğŸ“Š åˆ†ææ‘˜è¦</h2>
        <div class="metric">
            <strong>æ•°æ®è´¨é‡é£é™©:</strong>
            <span class="risk-{{ summary.data_quality_risk.lower() }}">
                {{ summary.data_quality_risk }}
            </span>
        </div>
        <div class="metric">
            <strong>æœ€ä½³æ‹Ÿåˆåˆ†å¸ƒ:</strong> {{ summary.best_fit_distribution }}
        </div>
        <div class="metric">
            <strong>å¼‚å¸¸å€¼æ•°é‡:</strong> {{ summary.anomaly_count }}
        </div>
        <div class="metric">
            <strong>æ•°æ®ç‚¹æ€»æ•°:</strong> {{ summary.total_data_points }}
        </div>
    </div>

    {% if recommendations %}
    <div class="section">
        <h2>ğŸ’¡ å»ºè®®</h2>
        <ul>
        {% for rec in recommendations %}
            <li>{{ rec }}</li>
        {% endfor %}
        </ul>
    </div>
    {% endif %}

    <div class="section">
        <h2>ğŸ¯ è¯¦ç»†åˆ†æç»“æœ</h2>
        
        {% if detailed_results.quality_assessment %}
        <h3>æ•°æ®è´¨é‡è¯„ä¼°</h3>
        <table>
            <tr><th>æŒ‡æ ‡</th><th>å€¼</th></tr>
            <tr><td>å¡æ–¹ç»Ÿè®¡é‡</td><td>{{ detailed_results.quality_assessment.statistics.chi_square }}</td></tr>
            <tr><td>på€¼</td><td>{{ detailed_results.quality_assessment.statistics.p_value }}</td></tr>
            <tr><td>MAD</td><td>{{ detailed_results.quality_assessment.statistics.mad }}</td></tr>
            <tr><td>ç»“è®º</td><td>{{ detailed_results.quality_assessment.verdict }}</td></tr>
        </table>
        {% endif %}

        {% if detailed_results.anomaly_detection and detailed_results.anomaly_detection.outliers %}
        <h3>ğŸš¨ å¼‚å¸¸å€¼æ£€æµ‹</h3>
        <table>
            <tr><th>ç´¢å¼•</th><th>å€¼</th><th>å¼‚å¸¸åˆ†æ•°</th><th>æ–¹æ³•</th></tr>
            {% for outlier in detailed_results.anomaly_detection.outliers[:10] %}
            <tr>
                <td>{{ outlier.index }}</td>
                <td>{{ outlier.value }}</td>
                <td>{{ outlier.score }}</td>
                <td>{{ outlier.method }}</td>
            </tr>
            {% endfor %}
        </table>
        {% if detailed_results.anomaly_detection.outliers|length > 10 %}
        <p><em>æ˜¾ç¤ºå‰10ä¸ªå¼‚å¸¸å€¼ï¼Œæ€»è®¡{{ detailed_results.anomaly_detection.outliers|length }}ä¸ª</em></p>
        {% endif %}
        {% endif %}
    </div>

    <div class="section">
        <h2>ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨</h2>
        <p><em>è¯·æŸ¥çœ‹åŒç›®å½•ä¸‹çš„PNGå›¾åƒæ–‡ä»¶ä»¥è·å–è¯¦ç»†çš„å¯è§†åŒ–åˆ†æã€‚</em></p>
    </div>

    <div class="section">
        <h2>ğŸ”§ æŠ€æœ¯ç»†èŠ‚</h2>
        <details>
            <summary>ç‚¹å‡»æŸ¥çœ‹å®Œæ•´JSONç»“æœ</summary>
            <pre style="background: #f4f4f4; padding: 15px; overflow-x: auto;">
{{ raw_json }}
            </pre>
        </details>
    </div>
</body>
</html>
"""

def generate_html_report(result_file, output_file):
    """ç”ŸæˆHTMLæŠ¥å‘Š"""
    with open(result_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    template = jinja2.Template(HTML_TEMPLATE)
    
    html_content = template.render(
        timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        input_file=data.get('input_file', result_file),
        summary=data.get('summary', {}),
        recommendations=data.get('recommendations', []),
        detailed_results=data.get('detailed_results', {}),
        raw_json=json.dumps(data, indent=2, ensure_ascii=False)
    )
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆlawkit HTMLæŠ¥å‘Š')
    parser.add_argument('result_file', help='åˆ†æç»“æœJSONæ–‡ä»¶')
    parser.add_argument('--output', help='è¾“å‡ºHTMLæ–‡ä»¶å')
    
    args = parser.parse_args()
    
    if not args.output:
        args.output = args.result_file.replace('.json', '_report.html')
    
    generate_html_report(args.result_file, args.output)

if __name__ == "__main__":
    main()
```

## ä¸‹ä¸€æ­¥

- æŸ¥çœ‹[é›†æˆæŒ‡å—](integrations_zh.md)äº†è§£ç³»ç»Ÿé›†æˆæ–¹æ³•
- å‚è€ƒ[æ€§èƒ½ä¼˜åŒ–æŒ‡å—](performance_zh.md)äº†è§£å¤§è§„æ¨¡ä¼˜åŒ–æŠ€å·§
- é˜…è¯»[CLIå‚è€ƒæ–‡æ¡£](../reference/cli-reference_zh.md)äº†è§£æ‰€æœ‰å¯ç”¨å‘½ä»¤